#!/bin/sh
# script to manage nfs in a clustered environment

. $CTDB_BASE/functions
loadconfig ctdb
loadconfig nfs

[ "$CTDB_MANAGES_NFS" = "yes" ] || exit 0
[ -z "$STATD_SHARED_DIRECTORY" ] && exit 0

cmd="$1"
shift

PATH=/usr/bin:/bin:/usr/sbin:/sbin:$PATH



case $cmd in 
     startup)
	/bin/mkdir -p $CTDB_BASE/state/nfs
	/bin/mkdir -p $CTDB_BASE/state/statd/ip
	/bin/mkdir -p $STATD_SHARED_DIRECTORY

	/bin/rm -f $CTDB_BASE/state/statd/statd.restart >/dev/null 2>/dev/null

	# make sure nfs is stopped before we start it, or it may get a bind error
	startstop_nfs stop
	startstop_nfs start
	;;

     shutdown)
	startstop_nfs stop
	exit 0
	;;

     takeip)
	ip=$2

	echo $ip >> $CTDB_BASE/state/statd/restart

	# having a list of what IPs we have allows statd to do the right 
	# thing via $CTDB_BASE/statd-callout
	touch $CTDB_BASE/state/statd/ip/$ip
	exit 0
	;;

     releaseip)
	iface=$1
	ip=$2
	maskbits=$3

	echo $ip >> $CTDB_BASE/state/statd/restart
	/bin/rm -f $CTDB_BASE/state/statd/ip/$ip
	exit 0
	;;

     recovered)
	# if no IPs have changed then don't need to restart statd 
	[ -f $CTDB_BASE/state/statd/restart ] || exit 0;

	# always restart the lockmanager so that we start with a clusterwide
	# graceperiod when ip addresses has changed
	[ -x $CTDB_BASE/statd-callout ] && {
		$CTDB_BASE/statd-callout notify &
	} >/dev/null 2>&1

	/bin/rm -f $CTDB_BASE/state/statd/restart
	;;

      monitor)
	# check how many times in a row that nfsd has stopped responding
	# after 3 times in a row we try to restart the full nfs service
	NFSD_FAIL_COUNT_FILE=$CTDB_BASE/state/nfs/nfsd_fail_count
	[ ! -f $NFSD_FAIL_COUNT_FILE ] && {
		echo 0 > $NFSD_FAIL_COUNT_FILE
	}
	NFSD_FAIL_COUNT=`cat $NFSD_FAIL_COUNT_FILE`
	[ -z "$NFSD_FAIL_COUNT" ] && {
		echo 0 > $NFSD_FAIL_COUNT_FILE
		NFSD_FAIL_COUNT=`cat $NFSD_FAIL_COUNT_FILE`
	}
	# ok it has failed a few times too many. try restarting it.
	[ `expr "$NFSD_FAIL_COUNT" ">" "3"` != "0" ] && {
		echo 60.nfs NFSD: trying to restart NFSD...
		echo 0 > $NFSD_FAIL_COUNT_FILE
		service nfs restart
		exit 0
	}
	expr "$NFSD_FAIL_COUNT" "+" "1" > $NFSD_FAIL_COUNT_FILE


	# check that statd responds to rpc requests
	# if statd is not running we try to restart it once and wait
	# for the next monitoring event to verify if it is running or not
	# if it still fails we fail and mark the node as UNHEALTHY
	if [ -f $CTDB_BASE/state/statd/statd.restart ]; then
		# statd was restarted, see if it came up ok
		rpcinfo -u localhost 100024 1 > /dev/null || {
			echo "ERROR: Failed to restart STATD"
			exit 1
		}
		echo "STATD successfully restarted."
		/bin/rm -f $CTDB_BASE/state/statd/statd.restart
	else
		rpcinfo -u localhost 100024 1 > /dev/null || {
			RPCSTATDOPTS=""
			[ -n "$STATD_HOSTNAME" ] && RPCSTATDOPTS="$RPCSTATDOPTS -n $STATD_HOSTNAME"
			[ -n "$STATD_PORT" ] && RPCSTATDOPTS="$RPCSTATDOPTS -p $STATD_PORT"
			[ -n "$STATD_OUTGOING_PORT" ] && RPCSTATDOPTS="$RPCSTATDOPTS -o $STATD_OUTGOING_PORT"
			rpc.statd $RPCSTATDOPTS 
			echo "ERROR: STATD is not responding. Trying to restart it. [rpc.statd $RPCSTATDOPTS]"
			touch $CTDB_BASE/state/statd/statd.restart
		}
	fi



	# check that NFS responds to rpc requests
	ctdb_check_rpc "NFS" 100003 3

	# and that its directories are available
	nfs_dirs=$(exportfs | grep -v '^#' | grep '^/' | awk {'print $1;'})
	ctdb_check_directories "nfs" $nfs_dirs

	# check that lockd responds to rpc requests
	ctdb_check_rpc "lockd" 100021 1
	ctdb_check_directories "statd" $STATD_SHARED_DIRECTORY

	# mount needs special handling since it is sometimes not started
	# correctly on RHEL5
	rpcinfo -u localhost 100005 1 > /dev/null || {
		echo "ERROR: MOUNTD is not running. Trying to restart it."
		RPCMOUNTDOPTS=""
		[ -n "$MOUNTD_PORT" ] && RPCMOUNTDOPTS="$RPCMOUNTDOPTS -p $MOUNTD_PORT"
		killall -q -9 rpc.mountd
		rpc.mountd $RPCMOUNTDOPTS &
		exit 1
	}
	
	# everything was ok with nfs so reset the fail count back to 0
	echo 0 > $NFSD_FAIL_COUNT_FILE
	;;

esac

exit 0
