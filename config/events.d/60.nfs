#!/bin/sh
# script to manage nfs in a clustered environment

. $CTDB_BASE/functions
loadconfig nfs

[ "$CTDB_MANAGES_NFS" = "yes" ] || exit 0
[ -z "$STATD_SHARED_DIRECTORY" ] && exit 0

cmd="$1"
shift

PATH=/usr/bin:/bin:/usr/sbin:/sbin:$PATH

case $cmd in 
     startup)
	/bin/mkdir -p $CTDB_BASE/state/nfs
	/bin/mkdir -p $CTDB_BASE/state/statd/ip
	/bin/mkdir -p $STATD_SHARED_DIRECTORY

	# make sure nfs is stopped before we start it, or it may get a bind error
	service nfs stop > /dev/null 2>&1
	service nfslock stop > /dev/null 2>&1

	service nfslock start
	service nfs start
	;;

     shutdown)
	# shutting down nfs can take a while so put it in the background
	service nfs stop &
	service nfslock stop &
	exit 0
	;;

     takeip)
	ip=$2

	echo $ip >> $CTDB_BASE/state/statd/restart

	# having a list of what IPs we have allows statd to do the right 
	# thing via $CTDB_BASE/statd-callout
	/bin/touch $CTDB_BASE/state/statd/ip/$ip
	exit 0
	;;

     releaseip)
	iface=$1
	ip=$2
	maskbits=$3

	echo $ip >> $CTDB_BASE/state/statd/restart
	/bin/rm -f $CTDB_BASE/state/statd/ip/$ip
	exit 0
	;;

     recovered)
	# always restart the lockmanager so that we start with a clusterwide
	# graceperiod when ip addresses has changed
	[ -x $CTDB_BASE/statd-callout ] && {
		$CTDB_BASE/statd-callout notify &
	} >/dev/null 2>&1

	/bin/rm -f $CTDB_BASE/state/statd/restart
	;;

      monitor)
	# check that NFS responds to rpc requests
	ctdb_check_rpc "NFS" 100003 3
	ctdb_check_rpc "mount" 100005 1

	# and that its directories are available
	nfs_dirs=`grep -v '^#' < /etc/exports | cut -d' ' -f1`
	ctdb_check_directories "nfs" $nfs_dirs

	# check that lockd responds to rpc requests
	ctdb_check_rpc "statd" 100024 1
	ctdb_check_rpc "lockd" 100021 1
	ctdb_check_directories "statd" $STATD_SHARED_DIRECTORY
	;;

esac

exit 0
